{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AiDM 2019: Assignment 1 - Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auke Bruinsma, s1594443 and Simon van Wageningen, s2317079"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As instructed, we have divided the first assignment of AiDM into two parts: The first on the naive models and this part on the matrix factorization. The structure of this part will be as follows: Before each cell of code we'll provide background info in markdown cells if necessary. If a cells produces relevant output we'll write this down. At the end there will be a short conclusion/discussion section where we compare our results of the naive models and matrix factorization with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we import the three datasets using pandas. We have given each columns the names that are found in the readme file so that each column can be accesed with, for example, ratings['UserID']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/local/lib64/python3.7/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/software/local/lib64/python3.7/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/software/local/lib64/python3.7/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Import datasets and infer a header with the correct column labels.\n",
    "ratings = pd.read_csv(filepath_or_buffer='data/ml-1m/ratings.dat',sep='::',header=None,names=['UserID','MovieID','Rating','Timestamp'])\n",
    "users = pd.read_csv(filepath_or_buffer='data/ml-1m/users.dat',sep='::',header=None,names=['UserID','Gender','Age','Occupation','Zip-code'])\n",
    "movies = pd.read_csv(filepath_or_buffer='data/ml-1m/movies.dat',sep='::',header=None,names=['MovieID','Title','Genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create the array $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported all necessary packages and datasets, we can start with the assignment. After a quick look at the data, we noticed the following things:\n",
    "\n",
    "- There are approximately 6000 users.\n",
    "- There are approximately 4000 movies.\n",
    "- Not every users has rated each movie.\n",
    "- Sometimes the ['MovieID'] column skips a number. This may mean that a movie is deleted from the database, or that no one has rated that movie. \n",
    "\n",
    "In order to complete the matrix factorization, we will create a matrix called $X$. This matrix has approximately the size $(6000,4000)$, where $X_{i,j}$ denotes how the $i_{th}$ user will rate the $j_{th}$ movie. Because of the last two points in the previous enumeration, there are a lot of empty elements in this matrix. Approximately 4.2 percent of the matrix $X$ has elements that are not equal to zero, which means 4.2 percent has been rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array X with at X[i][j] the ratings for how the ith user would rate the jth movies\n",
    "X = np.zeros((users['UserID'][len(users)-1]+1,movies['MovieID'][len(movies)-1]+1))\n",
    "\n",
    "# Fill array.\n",
    "for k in range(len(ratings)):\n",
    "    X[ratings['UserID'][k]][ratings['MovieID'][k]] = ratings['Rating'][k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we compute the percentage filled of the matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage filled: 4.188467095557036 %\n"
     ]
    }
   ],
   "source": [
    "# To get a better ovewview, here we print the number of matrix elements that is known.\n",
    "print('Percentage filled: {0} %'.format(100*len(ratings)/(len(X)*len(X[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact shape of the matrix $X$ is (6041,3953)$ and it also printed for a quick overview. You can see that that most elements are zero and only two elements have a rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (6041, 3953)\n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 5. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 3. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('Shape X: {0}\\n'.format(X.shape))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the imported data in a matrix $X$. This matrix will form the basis of the computations that are needed to perform the matrix factorization. However, first we need to set up the N-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. N-fold cross-validation:\n",
    "\n",
    "1. Split your data in $N$ parts (equal size);\n",
    "2. Develop $N$ models on all combinations of $(N-1)$ parts;\n",
    "3. Test each model on the remaining parts (test sets);\n",
    "4. Average the errors over these $N$ test sets;\n",
    "5. The average error is a realistic estimator of the error made by your model on the fresh data.\n",
    "\n",
    "#### In practice:\n",
    "- $N=5$ (5-fold cross-validation) or $(N=10)$ (10-fold cross-validation)\n",
    "- Errors also measured on the training sets/folds\n",
    "- Standard deviation of errors says something about 'model stability'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text in the previous cell is taken from the slides. The idea of N-fold cross validation is to create your own training and testing set so that you can test the accuracy of your model. You divide all your data in $N$ parts, in our case 5 parts. Then 4/5 is used as a training set and 1/5 as a testing set. Yobu repeat this process for each one-fifth of the data, so that every one-fifth is used for testing, and each 4/5 is used for the training set. This will give an accurate outcome of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The splitting of the data in 5 parts is done in the following way: All the indices of the elements of the matrix $X$ which are not equal to zero, are stored in an list called non_zero_indices. Each element of this array contains the $i$-coordinate, $j$-coordinate, and also the rating. This means the size of this array is $(1000209,3)$, since there are $1000209$ ratings present in the matrix $x$. Next, the non_zero_indices array is random shuffled, to make the training set a bit more valid. The array is then splitted into 5 parts, and this array is returned by the function k_fold_cross_validation. The size of this list is $(n=5,~10^6/(n=5),3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array where all the indices will be stored which are not zero.\n",
    "non_zero_indices = []\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X[i])):\n",
    "        if X[i][j] != 0:\n",
    "            non_zero_indices.append((i,j,X[i][j])) # Has length 1.000.209."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above coding cell creates the array with the indices of the elements that have a rating. The code cell below is the function that randomly shuffles the data and splits it into 5 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(data,k):\n",
    "    np.random.shuffle(data)\n",
    "    splitted_data = []\n",
    "    for i in range(k):\n",
    "        splitted_data.append(data[int(len(data)/k*i):int(len(data)/k*(i+1))])\n",
    "        print('Size set {0}: {1}'.format(i+1,np.shape(splitted_data[i])))\n",
    "    return splitted_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size set 1: (200041, 3)\n",
      "Size set 2: (200042, 3)\n",
      "Size set 3: (200042, 3)\n",
      "Size set 4: (200042, 3)\n",
      "Size set 5: (200042, 3)\n"
     ]
    }
   ],
   "source": [
    "folded_data = k_fold_cross_validation(non_zero_indices,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is some output that displays the size of each of the 5 sets. Each set contains approximately $2*10^5$ datapoints/ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have set up our data in 5 different parts. Next is the actual process called matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following equations are taken from the gravity-tikk paper page 24.\n",
    "\n",
    "The matrix $X$ can be approximated as the product of two matrices:\n",
    "\n",
    "$$X \\approx UM $$\n",
    "\n",
    "$X$ has shape $(i,j)$ where $i$ denotes the $i$th user and $j$ denotes the $j$th movie. The actual element $(i,j)$ contains the rating the user has given the movie. The matrices $U$ and $M$ have shapes $(i,k)$ and $(k,j)$, respectively. $k$ is a factor which can be chosen. In the code this will simply be called $k$ and experimentation will be done with this value. Now, instead of having a single matrix with approximate size $(6000,4000)$, you have two matrices with sizes $(6000,k)$ and $(k,4000)$, which is much less data. The key idea is that the product of the matrices $U$ and $M$ will give accurate predictions of what rating a certain user will give to movie he hasn't rated yet, based on his/hers known ratings.\n",
    "\n",
    "How we do obtain the matrix elements of $U$ and $M$? We initialise the matrices with random weight produced by numpy.random.randn, which all values obtained from a gaussian distribution. The elements of $U$ and $M$ are then constantly updated over several iterations. They are computed with the following equations.\n",
    "\n",
    "$$ \\hat{x}_{ij} = \\sum_{k=1}^{K} = u_{ik}m_{kj} $$\n",
    "\n",
    "$ \\hat{x}_{ij} $ denotes the predicted values of what user $i$ will rate movie $j$. Next the error of the predicted value compared to the known value is computed. This is done only for values which are known, so for the total $1000209$ ratings given by the $6041$ users.\n",
    "\n",
    "$$ e_{ij} = x_{ij}-\\hat{x}_{ij} $$\n",
    "\n",
    "The total error of all these ratings is just the sum of the errors:\n",
    "\n",
    "$$ SE = \\sum_{ij} e^2_{ij} $$\n",
    "\n",
    "What you want, is to minimize the total error over time. The new values for the elements in the matrices $U$ and $M$ is computed with the two following algorithms:\n",
    "\n",
    "$$ u`_{ik}  = u_{ik} + \\eta \\cdot (2 e_{ij} \\cdot m_{kj} - \\lambda \\cdot u_{ik} ) $$\n",
    "$$ m`_{kj}  = m_{kj} + \\eta \\cdot (2 e_{ij} \\cdot u_{ik} - \\lambda \\cdot m_{kj} ) $$\n",
    "\n",
    "$ u`_{ik} $ is the updated parameter value. $\\eta$ is the learning rate, so it specifies how fast the algorithm learns. $\\lambda$ is the regularization factor to prevent large weights.\n",
    "\n",
    "After these computations are performed on the training set. The new matrices $U$ and $M$ are computed and the equations that compute the predicted matrix values $\\hat{x}_{ij}$ and the (total) error are computed on the test set. From this error the RMSE can be computed. This is just the square root of the mean of the total error. This value usually starts out high, around 3 are 4, then rapidly decreases in the first few iterations and then slowly drops to 0.9 or 0.8, depending on the initial parameter values for the learning rate and regularization value. The iteration process will be stopped of the error increases. That means no further improvement can be obtained using the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_fac(indices,X,test_set):\n",
    "    # Initial parameters. Set eta and lambda to some small positive values.\n",
    "    eta = 0.01 # Learning rate (eq. 6 gravity-tikk).\n",
    "    lamda = 0.1 # The regularization factor lambda (eq. 6 gravity-tikk).\n",
    "    k = 10 # The parameter that sets the shape of the matrices U and M.\n",
    "    n_iter = 100 # Number of maximum iterations.\n",
    "\n",
    "    # Initialise the two matrices (eq. 1 gravity-tikk)\n",
    "    U = np.random.randn(users['UserID'][len(users)-1]+1,k) # Initialise the weights of U ...\n",
    "    M = np.random.randn(k,movies['MovieID'][len(movies)-1]+1) # ... and M randomly.\n",
    "    \n",
    "    prev_RMSE = 10e10+1; RMSE = 10e10 # Set some initial values for the RMSE.\n",
    "    prev_RMSE_test = 10e10+1; RMSE_test = 10e10\n",
    "    \n",
    "    total_SE_test = []\n",
    "    \n",
    "    for n in range(n_iter):\n",
    "        if RMSE_test < prev_RMSE_test: # Only continue the loop if there is improvement.\n",
    "            prev_RMSE_test = RMSE_test\n",
    "            SE_test = 0; SE = 0 # The total squared error (which is equivalent to minimize RMSE).\n",
    "            for i in range(len(indices)): # Loop until the terminal condition is met.\n",
    "                if i != test_set: # Don't use the testing set for training.\n",
    "                    for j in range(len(indices[i])):\n",
    "                        x_hat_ij = np.dot(U[indices[i][j][0],:],M[:,indices[i][j][1]]) # Eq. 3 gravity-tikk\n",
    "                        e_ij = X[indices[i][j][0]][indices[i][j][1]]-x_hat_ij # Eq. 4 gravity-tikk\n",
    "                        SE += e_ij**2; # Total error.\n",
    "                        temp = U[indices[i][j][0],:]+eta*(2*e_ij*M[:,indices[i][j][1]]-lamda*U[indices[i][j][0],:])\n",
    "                        M[:,indices[i][j][1]] += eta*(2*e_ij*U[indices[i][j][0],:]-lamda*M[:,indices[i][j][1]])\n",
    "                        U[indices[i][j][0],:] = temp # Update factorised matrices.\n",
    "            X_pred = np.dot(U,M)\n",
    "            for j in range(len(indices[test_set])):\n",
    "                SE_test += (X[indices[test_set][j][0]][indices[test_set][j][1]]-X_pred[indices[test_set][j][0]][indices[test_set][j][1]])**2  \n",
    "                RMSE_test = (SE_test/len(indices[test_set]))**0.5\n",
    "            RMSE = np.sqrt(SE/800168) # Compute the root mean squared error. (Use a fixed number for efficiency.)\n",
    "            total_SE_test.append(SE_test) # For statistics.\n",
    "            sys.stdout.write('\\rIterations: {0}\\n'.format(n)) # Overview of the process.\n",
    "            sys.stdout.write('\\rSE_train: {0}. RMSE_train: {1}\\n'.format(SE,RMSE)) # Overview of the process.\n",
    "            sys.stdout.write('\\rSE_test:  {0}. RMSE_test:  {1}\\n\\n'.format(SE_test,RMSE_test)) # Overview of the process.        \n",
    "        else:\n",
    "            print('\\nError increased. Process terminated.\\n')\n",
    "            break;\n",
    "            \n",
    "    return U,M,total_SE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0\n",
      "SE_train: 14064515.323242577. RMSE_train: 4.19248768560201\n",
      "SE_test:  2899222.06207733. RMSE_test:  3.806985581381573\n",
      "\n",
      "Iterations: 1\n",
      "SE_train: 7620082.536808458. RMSE_train: 3.085952578915223\n",
      "SE_test:  1107361.1546118236. RMSE_test:  2.3528006631485407\n",
      "\n",
      "Iterations: 2\n",
      "SE_train: 2912483.3763803593. RMSE_train: 1.9078364327441928\n",
      "SE_test:  562710.1741828651. RMSE_test:  1.677192359629935\n",
      "\n",
      "Iterations: 3\n",
      "SE_train: 1693423.4522644007. RMSE_train: 1.4547628277505065\n",
      "SE_test:  393731.3231178715. RMSE_test:  1.402944447830704\n",
      "\n",
      "Iterations: 4\n",
      "SE_train: 1253417.0734061184. RMSE_train: 1.2515759620798044\n",
      "SE_test:  318899.61216077214. RMSE_test:  1.2626049483890212\n",
      "\n",
      "Iterations: 5\n",
      "SE_train: 1043114.7802550715. RMSE_train: 1.14176167179436\n",
      "SE_test:  278771.76077781897. RMSE_test:  1.1804969806819532\n",
      "\n",
      "Iterations: 6\n",
      "SE_train: 925903.1308480541. RMSE_train: 1.075702521619204\n",
      "SE_test:  254630.56191044566. RMSE_test:  1.128225095767485\n",
      "\n",
      "Iterations: 7\n",
      "SE_train: 853834.196942427. RMSE_train: 1.0329901556932013\n",
      "SE_test:  238921.09813286655. RMSE_test:  1.0928680829504323\n",
      "\n",
      "Iterations: 8\n",
      "SE_train: 806268.8210991183. RMSE_train: 1.003804973709832\n",
      "SE_test:  228085.56161533244. RMSE_test:  1.0677987021403716\n",
      "\n",
      "Iterations: 9\n",
      "SE_train: 773107.7045968845. RMSE_train: 0.982945437235589\n",
      "SE_test:  220264.5227110184. RMSE_test:  1.0493316390412089\n",
      "\n",
      "Iterations: 10\n",
      "SE_train: 748947.4014533858. RMSE_train: 0.967464570101101\n",
      "SE_test:  214408.96910576112. RMSE_test:  1.0352898731171492\n",
      "\n",
      "Iterations: 11\n",
      "SE_train: 730694.3597577178. RMSE_train: 0.9556025233844774\n",
      "SE_test:  209890.79546450585. RMSE_test:  1.0243236223730476\n",
      "\n",
      "Iterations: 12\n",
      "SE_train: 716478.6349020593. RMSE_train: 0.9462611993913228\n",
      "SE_test:  206315.34006404234. RMSE_test:  1.0155615542347847\n",
      "\n",
      "Iterations: 13\n",
      "SE_train: 705117.7475300533. RMSE_train: 0.9387289970301839\n",
      "SE_test:  203424.463197524. RMSE_test:  1.0084214637980724\n",
      "\n",
      "Iterations: 14\n",
      "SE_train: 695834.7028317953. RMSE_train: 0.9325292275741739\n",
      "SE_test:  201043.4593508132. RMSE_test:  1.002502503460131\n",
      "\n",
      "Iterations: 15\n",
      "SE_train: 688101.7310752941. RMSE_train: 0.9273330443068198\n",
      "SE_test:  199050.5518586828. RMSE_test:  0.9975213152081932\n",
      "\n",
      "Iterations: 16\n",
      "SE_train: 681549.7879972634. RMSE_train: 0.9229075607772286\n",
      "SE_test:  197358.63790238556. RMSE_test:  0.9932728418564367\n",
      "\n",
      "Iterations: 17\n",
      "SE_train: 675914.1105838894. RMSE_train: 0.9190839178376347\n",
      "SE_test:  195903.9763878207. RMSE_test:  0.9896055383430269\n",
      "\n",
      "Iterations: 18\n",
      "SE_train: 671000.3601086461. RMSE_train: 0.9157370526097587\n",
      "SE_test:  194638.96204176053. RMSE_test:  0.9864052646670329\n",
      "\n",
      "Iterations: 19\n",
      "SE_train: 666662.9445963213. RMSE_train: 0.9127725447582898\n",
      "SE_test:  193527.38498493028. RMSE_test:  0.9835845667819565\n",
      "\n",
      "Iterations: 20\n",
      "SE_train: 662790.7761030503. RMSE_train: 0.9101178627401818\n",
      "SE_test:  192541.24668882808. RMSE_test:  0.9810753891091762\n",
      "\n",
      "Iterations: 21\n",
      "SE_train: 659297.6952630933. RMSE_train: 0.9077164148400176\n",
      "SE_test:  191658.57604837194. RMSE_test:  0.9788240242584028\n",
      "\n",
      "Iterations: 22\n",
      "SE_train: 656115.9003207331. RMSE_train: 0.9055234293699294\n",
      "SE_test:  190861.9031649354. RMSE_test:  0.976787552372864\n",
      "\n",
      "Iterations: 23\n",
      "SE_train: 653191.354565852. RMSE_train: 0.9035030528317848\n",
      "SE_test:  190137.17473858033. RMSE_test:  0.9749312914422122\n",
      "\n",
      "Iterations: 24\n",
      "SE_train: 650480.5232894606. RMSE_train: 0.9016262744755571\n",
      "SE_test:  189472.97152709507. RMSE_test:  0.9732269456013372\n",
      "\n",
      "Iterations: 25\n",
      "SE_train: 647948.0204407967. RMSE_train: 0.8998694210307769\n",
      "SE_test:  188859.93583319362. RMSE_test:  0.9716512426831962\n",
      "\n",
      "Iterations: 26\n",
      "SE_train: 645564.8874365884. RMSE_train: 0.8982130505019821\n",
      "SE_test:  188290.34709097503. RMSE_test:  0.9701849191806204\n",
      "\n",
      "Iterations: 27\n",
      "SE_train: 643307.3167917919. RMSE_train: 0.8966411284755749\n",
      "SE_test:  187757.80309200098. RMSE_test:  0.9688119544867094\n",
      "\n",
      "Iterations: 28\n",
      "SE_train: 641155.6916288557. RMSE_train: 0.8951404060276265\n",
      "SE_test:  187256.97722960924. RMSE_test:  0.9675189853877254\n",
      "\n",
      "Iterations: 29\n",
      "SE_train: 639093.8507156012. RMSE_train: 0.8936999420842212\n",
      "SE_test:  186783.43078044703. RMSE_test:  0.9662948515466226\n",
      "\n",
      "Iterations: 30\n",
      "SE_train: 637108.5148126081. RMSE_train: 0.8923107293111828\n",
      "SE_test:  186333.46519073696. RMSE_test:  0.9651302364433304\n",
      "\n",
      "Iterations: 31\n",
      "SE_train: 635188.8283050361. RMSE_train: 0.8909653940088232\n",
      "SE_test:  185904.00353305586. RMSE_test:  0.9640173780159413\n",
      "\n",
      "Iterations: 32\n",
      "SE_train: 633325.9831670872. RMSE_train: 0.8896579487606697\n",
      "SE_test:  185492.4933404116. RMSE_test:  0.9629498303987626\n",
      "\n",
      "Iterations: 33\n",
      "SE_train: 631512.9019980637. RMSE_train: 0.8883835827871003\n",
      "SE_test:  185096.82527590162. RMSE_test:  0.9619222635011101\n",
      "\n",
      "Iterations: 34\n",
      "SE_train: 629743.9642137303. RMSE_train: 0.8871384797109774\n",
      "SE_test:  184715.26378460086. RMSE_test:  0.9609302912193812\n",
      "\n",
      "Iterations: 35\n",
      "SE_train: 628014.7650710569. RMSE_train: 0.8859196561060351\n",
      "SE_test:  184346.38713371384. RMSE_test:  0.9599703221191802\n",
      "\n",
      "Iterations: 36\n",
      "SE_train: 626321.9013803109. RMSE_train: 0.8847248169600659\n",
      "SE_test:  183989.03516128586. RMSE_test:  0.9590394286462686\n",
      "\n",
      "Iterations: 37\n",
      "SE_train: 624662.7806979268. RMSE_train: 0.8835522261416023\n",
      "SE_test:  183642.26368044. RMSE_test:  0.9581352324499133\n",
      "\n",
      "Iterations: 38\n",
      "SE_train: 623035.4526523326. RMSE_train: 0.8824005911980753\n",
      "SE_test:  183305.3048747964. RMSE_test:  0.9572558043415659\n",
      "\n",
      "Iterations: 39\n",
      "SE_train: 621438.461987107. RMSE_train: 0.8812689624270305\n",
      "SE_test:  182977.53322294378. RMSE_test:  0.9563995778875267\n",
      "\n",
      "Iterations: 40\n",
      "SE_train: 619870.7231185843. RMSE_train: 0.8801566462832687\n",
      "SE_test:  182658.4365585564. RMSE_test:  0.9555652757758346\n",
      "\n",
      "Iterations: 41\n",
      "SE_train: 618331.4157235443. RMSE_train: 0.8790631329638195\n",
      "SE_test:  182347.59186165803. RMSE_test:  0.9547518480393565\n",
      "\n",
      "Iterations: 42\n",
      "SE_train: 616819.9003342924. RMSE_train: 0.87798803761161\n",
      "SE_test:  182044.64533041455. RMSE_test:  0.9539584210723757\n",
      "\n",
      "Iterations: 43\n",
      "SE_train: 615335.6523287579. RMSE_train: 0.8769310541402141\n",
      "SE_test:  181749.2962377999. RMSE_test:  0.9531842562393691\n",
      "\n",
      "Iterations: 44\n",
      "SE_train: 613878.2122123821. RMSE_train: 0.8758919203119203\n",
      "SE_test:  181461.2840532901. RMSE_test:  0.9524287167930381\n",
      "\n",
      "Iterations: 45\n",
      "SE_train: 612447.1497831095. RMSE_train: 0.8748703924617233\n",
      "SE_test:  181180.3783145938. RMSE_test:  0.9516912418152857\n",
      "\n",
      "Iterations: 46\n",
      "SE_train: 611042.0396780615. RMSE_train: 0.8738662281715559\n",
      "SE_test:  180906.3707668399. RMSE_test:  0.9509713259658903\n",
      "\n",
      "Iterations: 47\n",
      "SE_train: 609662.4458907408. RMSE_train: 0.872879175242465\n",
      "SE_test:  180639.06933758044. RMSE_test:  0.9502685039455091\n",
      "\n",
      "Iterations: 48\n",
      "SE_train: 608307.9130836299. RMSE_train: 0.8719089654621555\n",
      "SE_test:  180378.2935779551. RMSE_test:  0.9495823387327077\n",
      "\n",
      "Iterations: 49\n",
      "SE_train: 606977.9628317194. RMSE_train: 0.8709553118714192\n",
      "SE_test:  180123.8712623229. RMSE_test:  0.9489124128097081\n",
      "\n",
      "Iterations: 50\n",
      "SE_train: 605672.093277568. RMSE_train: 0.8700179084669113\n",
      "SE_test:  179875.63589749648. RMSE_test:  0.9482583217400777\n",
      "\n",
      "Iterations: 51\n",
      "SE_train: 604389.7810098802. RMSE_train: 0.8690964315050584\n",
      "SE_test:  179633.42494223564. RMSE_test:  0.9476196695870921\n",
      "\n",
      "Iterations: 52\n",
      "SE_train: 603130.4842740739. RMSE_train: 0.8681905417770098\n",
      "SE_test:  179397.0785791441. RMSE_test:  0.9469960657670452\n",
      "\n",
      "Iterations: 53\n",
      "SE_train: 601893.6468710529. RMSE_train: 0.8672998873970489\n",
      "SE_test:  179166.4389135771. RMSE_test:  0.9463871230144834\n",
      "\n",
      "Iterations: 54\n",
      "SE_train: 600678.7022969961. RMSE_train: 0.8664241067845201\n",
      "SE_test:  178941.3494997644. RMSE_test:  0.945792456201639\n",
      "\n",
      "Iterations: 55\n",
      "SE_train: 599485.0778291725. RMSE_train: 0.865562831626458\n",
      "SE_test:  178721.6551139885. RMSE_test:  0.9452116818044736\n",
      "\n",
      "Iterations: 56\n",
      "SE_train: 598312.1983711732. RMSE_train: 0.8647156896847248\n",
      "SE_test:  178507.20171069502. RMSE_test:  0.9446444178487796\n",
      "\n",
      "Iterations: 57\n",
      "SE_train: 597159.4899504839. RMSE_train: 0.8638823073680224\n",
      "SE_test:  178297.83650939018. RMSE_test:  0.9440902842004482\n",
      "\n",
      "Iterations: 58\n",
      "SE_train: 596026.3828169199. RMSE_train: 0.8630623120289268\n",
      "SE_test:  178093.40817094338. RMSE_test:  0.9435489030916994\n",
      "\n",
      "Iterations: 59\n",
      "SE_train: 594912.3141231947. RMSE_train: 0.862255333969696\n",
      "SE_test:  177893.7670302265. RMSE_test:  0.9430198997964958\n",
      "\n",
      "Iterations: 60\n",
      "SE_train: 593816.7301963238. RMSE_train: 0.8614610081605567\n",
      "SE_test:  177698.7653590978. RMSE_test:  0.9425029033866665\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 61\n",
      "SE_train: 592739.0884203017. RMSE_train: 0.86067897568289\n",
      "SE_test:  177508.2576400182. RMSE_test:  0.9419975475165994\n",
      "\n",
      "Iterations: 62\n",
      "SE_train: 591678.8587589926. RMSE_train: 0.859908884916167\n",
      "SE_test:  177322.10083544877. RMSE_test:  0.9415034711970303\n",
      "\n",
      "Iterations: 63\n",
      "SE_train: 590635.5249540439. RMSE_train: 0.8591503924920257\n",
      "SE_test:  177140.15464233307. RMSE_test:  0.9410203195293411\n",
      "\n",
      "Iterations: 64\n",
      "SE_train: 589608.5854315872. RMSE_train: 0.8584031640384123\n",
      "SE_test:  176962.28172442308. RMSE_test:  0.9405477443808613\n",
      "\n",
      "Iterations: 65\n",
      "SE_train: 588597.5539537853. RMSE_train: 0.8576668747386403\n",
      "SE_test:  176788.34791787594. RMSE_test:  0.940085404988718\n",
      "\n",
      "Iterations: 66\n",
      "SE_train: 587601.9600471273. RMSE_train: 0.856941209727455\n",
      "SE_test:  176618.2224075191. RMSE_test:  0.939632968484996\n",
      "\n",
      "Iterations: 67\n",
      "SE_train: 586621.3492397607. RMSE_train: 0.8562258643466908\n",
      "SE_test:  176451.7778730104. RMSE_test:  0.9391901103408525\n",
      "\n",
      "Iterations: 68\n",
      "SE_train: 585655.2831361599. RMSE_train: 0.8555205442804081\n",
      "SE_test:  176288.89060502534. RMSE_test:  0.9387565147296543\n",
      "\n",
      "Iterations: 69\n",
      "SE_train: 584703.3393549396. RMSE_train: 0.8548249655877501\n",
      "SE_test:  176129.44059264433. RMSE_test:  0.9383318748120132\n",
      "\n",
      "Iterations: 70\n",
      "SE_train: 583765.1113538218. RMSE_train: 0.854138854650602\n",
      "SE_test:  175973.31158356467. RMSE_test:  0.9379158929468375\n",
      "\n",
      "Iterations: 71\n",
      "SE_train: 582840.20816192. RMSE_train: 0.8534619480504554\n",
      "SE_test:  175820.39111910108. RMSE_test:  0.9375082808334527\n",
      "\n",
      "Iterations: 72\n",
      "SE_train: 581928.2540378153. RMSE_train: 0.8527939923877537\n",
      "SE_test:  175670.5705462975. RMSE_test:  0.9371087595908442\n",
      "\n",
      "Iterations: 73\n",
      "SE_train: 581028.8880697404. RMSE_train: 0.8521347440555102\n",
      "SE_test:  175523.74500932737. RMSE_test:  0.9367170597797234\n",
      "\n",
      "Iterations: 74\n",
      "SE_train: 580141.7637311168. RMSE_train: 0.8514839689768192\n",
      "SE_test:  175379.81342251875. RMSE_test:  0.936332921373571\n",
      "\n",
      "Iterations: 75\n",
      "SE_train: 579266.5484033127. RMSE_train: 0.8508414423149293\n",
      "SE_test:  175238.67842722486. RMSE_test:  0.9359560936845365\n",
      "\n",
      "Iterations: 76\n",
      "SE_train: 578402.9228762288. RMSE_train: 0.8502069481636796\n",
      "SE_test:  175100.2463346949. RMSE_test:  0.9355863352499222\n",
      "\n",
      "Iterations: 77\n",
      "SE_train: 577550.5808345792. RMSE_train: 0.8495802792241274\n",
      "SE_test:  174964.4270568838. RMSE_test:  0.935223413684426\n",
      "\n",
      "Iterations: 78\n",
      "SE_train: 576709.2283369458. RMSE_train: 0.8489612364726468\n",
      "SE_test:  174831.13402713393. RMSE_test:  0.9348671055033193\n",
      "\n",
      "Iterations: 79\n",
      "SE_train: 575878.5832942446. RMSE_train: 0.8483496288254818\n",
      "SE_test:  174700.28411248414. RMSE_test:  0.9345171959212836\n",
      "\n",
      "Iterations: 80\n",
      "SE_train: 575058.3749513179. RMSE_train: 0.847745272802603\n",
      "SE_test:  174571.79751901785. RMSE_test:  0.9341734786307161\n",
      "\n",
      "Iterations: 81\n",
      "SE_train: 574248.343376327. RMSE_train: 0.8471479921944371\n",
      "SE_test:  174445.59769197597. RMSE_test:  0.9338357555641752\n",
      "\n",
      "Iterations: 82\n",
      "SE_train: 573448.238960679. RMSE_train: 0.8465576177336182\n",
      "SE_test:  174321.6112116105. RMSE_test:  0.9335038366436325\n",
      "\n",
      "Iterations: 83\n",
      "SE_train: 572657.8219321113. RMSE_train: 0.8459739867738348\n",
      "SE_test:  174199.76768626747. RMSE_test:  0.9331775395205796\n",
      "\n",
      "Iterations: 84\n",
      "SE_train: 571876.861882478. RMSE_train: 0.8453969429770529\n",
      "SE_test:  174079.99964348655. RMSE_test:  0.9328566893091677\n",
      "\n",
      "Iterations: 85\n",
      "SE_train: 571105.137311978. RMSE_train: 0.8448263360105371\n",
      "SE_test:  173962.24242022002. RMSE_test:  0.9325411183154035\n",
      "\n",
      "Iterations: 86\n",
      "SE_train: 570342.4351906622. RMSE_train: 0.8442620212544302\n",
      "SE_test:  173846.43405289963. RMSE_test:  0.9322306657644281\n",
      "\n",
      "Iterations: 87\n",
      "SE_train: 569588.5505376842. RMSE_train: 0.8437038595203685\n",
      "SE_test:  173732.51516806486. RMSE_test:  0.9319251775278633\n",
      "\n",
      "Iterations: 88\n",
      "SE_train: 568843.2860189585. RMSE_train: 0.8431517167817549\n",
      "SE_test:  173620.42887429267. RMSE_test:  0.9316245058532877\n",
      "\n",
      "Iterations: 89\n",
      "SE_train: 568106.4515631457. RMSE_train: 0.8426054639157581\n",
      "SE_test:  173510.1206557074. RMSE_test:  0.9313285090966594\n",
      "\n",
      "Iterations: 90\n",
      "SE_train: 567377.863996024. RMSE_train: 0.8420649764572026\n",
      "SE_test:  173401.53826781426. RMSE_test:  0.9310370514597565\n",
      "\n",
      "Iterations: 91\n",
      "SE_train: 566657.3466932201. RMSE_train: 0.841530134364448\n",
      "SE_test:  173294.63163580562. RMSE_test:  0.9307500027331038\n",
      "\n",
      "Iterations: 92\n",
      "SE_train: 565944.7292503534. RMSE_train: 0.8410008217966655\n",
      "SE_test:  173189.35275572073. RMSE_test:  0.9304672380454795\n",
      "\n",
      "Iterations: 93\n",
      "SE_train: 565239.8471712196. RMSE_train: 0.8404769269030808\n",
      "SE_test:  173085.65559871492. RMSE_test:  0.9301886376207543\n",
      "\n",
      "Iterations: 94\n",
      "SE_train: 564542.5415724629. RMSE_train: 0.8399583416231364\n",
      "SE_test:  172983.49601857626. RMSE_test:  0.9299140865424985\n",
      "\n",
      "Iterations: 95\n",
      "SE_train: 563852.6589049876. RMSE_train: 0.8394449614978482\n",
      "SE_test:  172882.8316626697. RMSE_test:  0.9296434745269037\n",
      "\n",
      "Iterations: 96\n",
      "SE_train: 563170.0506910498. RMSE_train: 0.8389366854916627\n",
      "SE_test:  172783.6218863583. RMSE_test:  0.9293766957042098\n",
      "\n",
      "Iterations: 97\n",
      "SE_train: 562494.5732766443. RMSE_train: 0.8384334158246101\n",
      "SE_test:  172685.82767097186. RMSE_test:  0.9291136484088849\n",
      "\n",
      "Iterations: 98\n",
      "SE_train: 561826.0875984468. RMSE_train: 0.8379350578142822\n",
      "SE_test:  172589.41154528238. RMSE_test:  0.9288542349785004\n",
      "\n",
      "Iterations: 99\n",
      "SE_train: 561164.4589646354. RMSE_train: 0.8374415197272057\n",
      "SE_test:  172494.3375106035. RMSE_test:  0.9285983615616673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "U_0,M_0,SE_0 = mat_fac(folded_data,X,0) # Done with 0.001,0.01,10,100. Gives 0.928598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0\n",
      "SE_train: 3563131.281638583. RMSE_train: 2.110208277981861\n",
      "SE_test:  218363.4999660699. RMSE_test:  1.044793627046562\n",
      "\n",
      "Iterations: 1\n",
      "SE_train: 758722.2108931269. RMSE_train: 0.9737574856461078\n",
      "SE_test:  194973.27318762627. RMSE_test:  0.9872520242032813\n",
      "\n",
      "Iterations: 2\n",
      "SE_train: 712640.6723028921. RMSE_train: 0.9437233765665682\n",
      "SE_test:  188592.10549669852. RMSE_test:  0.9709620281942815\n",
      "\n",
      "Iterations: 3\n",
      "SE_train: 693177.3835320486. RMSE_train: 0.930746908968955\n",
      "SE_test:  184516.1861338203. RMSE_test:  0.9604123282954209\n",
      "\n",
      "Iterations: 4\n",
      "SE_train: 677328.8656294869. RMSE_train: 0.9200452817875571\n",
      "SE_test:  180999.01673359226. RMSE_test:  0.9512148010309935\n",
      "\n",
      "Iterations: 5\n",
      "SE_train: 662990.4625445812. RMSE_train: 0.9102549531444387\n",
      "SE_test:  178020.4685762454. RMSE_test:  0.9433556642644494\n",
      "\n",
      "Iterations: 6\n",
      "SE_train: 650482.453990434. RMSE_train: 0.9016276125397816\n",
      "SE_test:  175542.38310307692. RMSE_test:  0.9367667913899314\n",
      "\n",
      "Iterations: 7\n",
      "SE_train: 639624.8201966492. RMSE_train: 0.8940711151706139\n",
      "SE_test:  173458.641885889. RMSE_test:  0.9311903407800356\n",
      "\n",
      "Iterations: 8\n",
      "SE_train: 630178.0385136841. RMSE_train: 0.8874441735333362\n",
      "SE_test:  171697.29805732178. RMSE_test:  0.9264505041428821\n",
      "\n",
      "Iterations: 9\n",
      "SE_train: 621956.3304455386. RMSE_train: 0.8816360835391922\n",
      "SE_test:  170203.39982603633. RMSE_test:  0.9224112837568821\n",
      "\n",
      "Iterations: 10\n",
      "SE_train: 614759.0902048044. RMSE_train: 0.8765201209192556\n",
      "SE_test:  168924.8480768174. RMSE_test:  0.9189402199832516\n",
      "\n",
      "Iterations: 11\n",
      "SE_train: 608381.329086292. RMSE_train: 0.871961578734384\n",
      "SE_test:  167815.3036420367. RMSE_test:  0.9159173231132349\n",
      "\n",
      "Iterations: 12\n",
      "SE_train: 602650.1058470829. RMSE_train: 0.8678447268205188\n",
      "SE_test:  166838.19447222416. RMSE_test:  0.9132469536009836\n",
      "\n",
      "Iterations: 13\n",
      "SE_train: 597440.5469695863. RMSE_train: 0.8640855793816946\n",
      "SE_test:  165967.1388188249. RMSE_test:  0.910859820523187\n",
      "\n",
      "Iterations: 14\n",
      "SE_train: 592671.6781240032. RMSE_train: 0.8606300331755006\n",
      "SE_test:  165184.04961494313. RMSE_test:  0.9087084070955022\n",
      "\n",
      "Iterations: 15\n",
      "SE_train: 588293.3865364819. RMSE_train: 0.8574452394057014\n",
      "SE_test:  164476.63705304553. RMSE_test:  0.906760514836094\n",
      "\n",
      "Iterations: 16\n",
      "SE_train: 584273.1808684089. RMSE_train: 0.8545104660428858\n",
      "SE_test:  163836.24976985765. RMSE_test:  0.9049935641275634\n",
      "\n",
      "Iterations: 17\n",
      "SE_train: 580586.5151173071. RMSE_train: 0.8518102910475592\n",
      "SE_test:  163256.28958930436. RMSE_test:  0.90339036111024\n",
      "\n",
      "Iterations: 18\n",
      "SE_train: 577211.0306534033. RMSE_train: 0.8493305023565827\n",
      "SE_test:  162731.16768273283. RMSE_test:  0.9019362910725278\n",
      "\n",
      "Iterations: 19\n",
      "SE_train: 574123.9206559601. RMSE_train: 0.8470562112001271\n",
      "SE_test:  162255.73675142045. RMSE_test:  0.9006177911378006\n",
      "\n",
      "Iterations: 20\n",
      "SE_train: 571301.4417403948. RMSE_train: 0.8449715184746859\n",
      "SE_test:  161825.08839534235. RMSE_test:  0.8994218172012696\n",
      "\n",
      "Iterations: 21\n",
      "SE_train: 568719.5684687311. RMSE_train: 0.8430600233929224\n",
      "SE_test:  161434.55956462788. RMSE_test:  0.8983358844630832\n",
      "\n",
      "Iterations: 22\n",
      "SE_train: 566354.9508434918. RMSE_train: 0.8413055637481129\n",
      "SE_test:  161079.8088668798. RMSE_test:  0.8973482999205314\n",
      "\n",
      "Iterations: 23\n",
      "SE_train: 564185.6865525083. RMSE_train: 0.8396928251105409\n",
      "SE_test:  160756.88180697622. RMSE_test:  0.8964483627087204\n",
      "\n",
      "Iterations: 24\n",
      "SE_train: 562191.7557731533. RMSE_train: 0.8382077012082775\n",
      "SE_test:  160462.23782034218. RMSE_test:  0.8956264561848104\n",
      "\n",
      "Iterations: 25\n",
      "SE_train: 560355.1597348423. RMSE_train: 0.8368374318362051\n",
      "SE_test:  160192.74206734227. RMSE_test:  0.8948740394210201\n",
      "\n",
      "Iterations: 26\n",
      "SE_train: 558659.8661827975. RMSE_train: 0.8355705926510837\n",
      "SE_test:  159945.63512520381. RMSE_test:  0.8941835745257816\n",
      "\n",
      "Iterations: 27\n",
      "SE_train: 557091.6567474833. RMSE_train: 0.8343970068724407\n",
      "SE_test:  159718.49336239713. RMSE_test:  0.8935484256100835\n",
      "\n",
      "Iterations: 28\n",
      "SE_train: 555637.9411131606. RMSE_train: 0.8333076275032044\n",
      "SE_test:  159509.18879556557. RMSE_test:  0.8929627543012705\n",
      "\n",
      "Iterations: 29\n",
      "SE_train: 554287.5740370691. RMSE_train: 0.8322944176232584\n",
      "SE_test:  159315.85305168308. RMSE_test:  0.892421425100507\n",
      "\n",
      "Iterations: 30\n",
      "SE_train: 553030.6907871063. RMSE_train: 0.8313502411180065\n",
      "SE_test:  159136.84693051685. RMSE_test:  0.8919199251145009\n",
      "\n",
      "Iterations: 31\n",
      "SE_train: 551858.5644014415. RMSE_train: 0.830468767048745\n",
      "SE_test:  158970.73516718316. RMSE_test:  0.8914542973306309\n",
      "\n",
      "Iterations: 32\n",
      "SE_train: 550763.4821619553. RMSE_train: 0.8296443862540378\n",
      "SE_test:  158816.2651130621. RMSE_test:  0.8910210840704038\n",
      "\n",
      "Iterations: 33\n",
      "SE_train: 549738.6365887567. RMSE_train: 0.8288721370885989\n",
      "SE_test:  158672.34787427165. RMSE_test:  0.890617276695765\n",
      "\n",
      "Iterations: 34\n",
      "SE_train: 548778.0263996675. RMSE_train: 0.8281476372076972\n",
      "SE_test:  158538.04068913424. RMSE_test:  0.8902402682758981\n",
      "\n",
      "Iterations: 35\n",
      "SE_train: 547876.3641001312. RMSE_train: 0.8274670191463411\n",
      "SE_test:  158412.52976797012. RMSE_test:  0.8898878071287689\n",
      "\n",
      "Iterations: 36\n",
      "SE_train: 547028.9884311192. RMSE_train: 0.8268268685632305\n",
      "SE_test:  158295.11329969825. RMSE_test:  0.8895579504800699\n",
      "\n",
      "Iterations: 37\n",
      "SE_train: 546231.7813476424. RMSE_train: 0.8262241650706267\n",
      "SE_test:  158185.1847385888. RMSE_test:  0.8892490186165157\n",
      "\n",
      "Iterations: 38\n",
      "SE_train: 545481.0902495809. RMSE_train: 0.8256562263370376\n",
      "SE_test:  158082.21675719827. RMSE_test:  0.8889595506681015\n",
      "\n",
      "Iterations: 39\n",
      "SE_train: 544773.6567196966. RMSE_train: 0.825120656533985\n",
      "SE_test:  157985.74636492957. RMSE_test:  0.8886882634673279\n",
      "\n",
      "Iterations: 40\n",
      "SE_train: 544106.5530517926. RMSE_train: 0.824615300204975\n",
      "SE_test:  157895.3616616274. RMSE_test:  0.8884340148448824\n",
      "\n",
      "Iterations: 41\n",
      "SE_train: 543477.127486681. RMSE_train: 0.8241382023455002\n",
      "SE_test:  157810.69056248665. RMSE_test:  0.8881957723428894\n",
      "\n",
      "Iterations: 42\n",
      "SE_train: 542882.9584920568. RMSE_train: 0.8236875750298815\n",
      "SE_test:  157731.39164713572. RMSE_test:  0.8879725878064908\n",
      "\n",
      "Iterations: 43\n",
      "SE_train: 542321.8177995392. RMSE_train: 0.8232617704360291\n",
      "SE_test:  157657.14710150048. RMSE_test:  0.8877635777912054\n",
      "\n",
      "Iterations: 44\n",
      "SE_train: 541791.6413912723. RMSE_train: 0.8228592597116586\n",
      "SE_test:  157587.65756978217. RMSE_test:  0.8875679092927039\n",
      "\n",
      "Iterations: 45\n",
      "SE_train: 541290.5072876749. RMSE_train: 0.8224786168550554\n",
      "SE_test:  157522.63863513435. RMSE_test:  0.8873847900227563\n",
      "\n",
      "Iterations: 46\n",
      "SE_train: 540816.6188431452. RMSE_train: 0.8221185066629639\n",
      "SE_test:  157461.81860160007. RMSE_test:  0.887213462321113\n",
      "\n",
      "Iterations: 47\n",
      "SE_train: 540368.2922792124. RMSE_train: 0.8217776758064436\n",
      "SE_test:  157404.93724919623. RMSE_test:  0.8870531997874954\n",
      "\n",
      "Iterations: 48\n",
      "SE_train: 539943.9473223697. RMSE_train: 0.8214549461930039\n",
      "SE_test:  157351.7452645205. RMSE_test:  0.8869033058008439\n",
      "\n",
      "Iterations: 49\n",
      "SE_train: 539542.1000108274. RMSE_train: 0.8211492099175979\n",
      "SE_test:  157302.00409751743. RMSE_test:  0.8867631132268056\n",
      "\n",
      "Iterations: 50\n",
      "SE_train: 539161.3569458394. RMSE_train: 0.8208594252617988\n",
      "SE_test:  157255.48604932835. RMSE_test:  0.8866319847659331\n",
      "\n",
      "Iterations: 51\n",
      "SE_train: 538800.4104587557. RMSE_train: 0.8205846133463848\n",
      "SE_test:  157211.974448731. RMSE_test:  0.8865093135423003\n",
      "\n",
      "Iterations: 52\n",
      "SE_train: 538458.0343282188. RMSE_train: 0.8203238551648546\n",
      "SE_test:  157171.26382065727. RMSE_test:  0.8863945236612832\n",
      "\n",
      "Iterations: 53\n",
      "SE_train: 538133.0798090124. RMSE_train: 0.820076288820847\n",
      "SE_test:  157133.1599875433. RMSE_test:  0.8862870705700086\n",
      "\n",
      "Iterations: 54\n",
      "SE_train: 537824.471826941. RMSE_train: 0.8198411068622816\n",
      "SE_test:  157097.48007279515. RMSE_test:  0.8861864411342816\n",
      "\n",
      "Iterations: 55\n",
      "SE_train: 537531.205259049. RMSE_train: 0.8196175536539082\n",
      "SE_test:  157064.05239591832. RMSE_test:  0.8860921534029097\n",
      "\n",
      "Iterations: 56\n",
      "SE_train: 537252.3412602816. RMSE_train: 0.8194049227613833\n",
      "SE_test:  157032.7162623535. RMSE_test:  0.8860037560684126\n",
      "\n",
      "Iterations: 57\n",
      "SE_train: 536987.0036263084. RMSE_train: 0.8192025543414932\n",
      "SE_test:  157003.32165950435. RMSE_test:  0.8859208276569538\n",
      "\n",
      "Iterations: 58\n",
      "SE_train: 536734.3751967833. RMSE_train: 0.8190098325440057\n",
      "SE_test:  156975.7288744935. RMSE_test:  0.885842975491783\n",
      "\n",
      "Iterations: 59\n",
      "SE_train: 536493.6943134081. RMSE_train: 0.8188261829381182\n",
      "SE_test:  156949.80805096912. RMSE_test:  0.8857698344795404\n",
      "\n",
      "Iterations: 60\n",
      "SE_train: 536264.2513510319. RMSE_train: 0.8186510699792497\n",
      "SE_test:  156925.4387018764. RMSE_test:  0.8857010657676281\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 61\n",
      "SE_train: 536045.3853405264. RMSE_train: 0.8184839945321495\n",
      "SE_test:  156902.5091938025. RMSE_test:  0.8856363553171648\n",
      "\n",
      "Iterations: 62\n",
      "SE_train: 535836.4807025524. RMSE_train: 0.8183244914664294\n",
      "SE_test:  156880.9162166919. RMSE_test:  0.8855754124309141\n",
      "\n",
      "Iterations: 63\n",
      "SE_train: 535636.9641080613. RMSE_train: 0.8181721273379988\n",
      "SE_test:  156860.56425032482. RMSE_test:  0.8855179682687631\n",
      "\n",
      "Iterations: 64\n",
      "SE_train: 535446.3014791866. RMSE_train: 0.8180264981680759\n",
      "SE_test:  156841.36503705816. RMSE_test:  0.8854637743779612\n",
      "\n",
      "Iterations: 65\n",
      "SE_train: 535263.995141225. RMSE_train: 0.8178872273290734\n",
      "SE_test:  156823.23706817388. RMSE_test:  0.8854126012592114\n",
      "\n",
      "Iterations: 66\n",
      "SE_train: 535089.5811328542. RMSE_train: 0.8177539635438281\n",
      "SE_test:  156806.10508948102. RMSE_test:  0.8853642369848796\n",
      "\n",
      "Iterations: 67\n",
      "SE_train: 534922.6266785109. RMSE_train: 0.8176263790020808\n",
      "SE_test:  156789.89962992843. RMSE_test:  0.8853184858802208\n",
      "\n",
      "Iterations: 68\n",
      "SE_train: 534762.7278247107. RMSE_train: 0.8175041675963741\n",
      "SE_test:  156774.55655622968. RMSE_test:  0.885275167276356\n",
      "\n",
      "Iterations: 69\n",
      "SE_train: 534609.5072389785. RMSE_train: 0.8173870432770723\n",
      "SE_test:  156760.0166547717. RMSE_test:  0.8852341143388198\n",
      "\n",
      "Iterations: 70\n",
      "SE_train: 534462.6121677273. RMSE_train: 0.8172747385243415\n",
      "SE_test:  156746.2252415896. RMSE_test:  0.8851951729740879\n",
      "\n",
      "Iterations: 71\n",
      "SE_train: 534321.7125488364. RMSE_train: 0.8171670029344072\n",
      "SE_test:  156733.13180034023. RMSE_test:  0.885158200814072\n",
      "\n",
      "Iterations: 72\n",
      "SE_train: 534186.4992716965. RMSE_train: 0.817063601915059\n",
      "SE_test:  156720.6896475531. RMSE_test:  0.885123066276697\n",
      "\n",
      "Iterations: 73\n",
      "SE_train: 534056.6825780368. RMSE_train: 0.8169643154857272\n",
      "SE_test:  156708.85562432057. RMSE_test:  0.885089647700326\n",
      "\n",
      "Iterations: 74\n",
      "SE_train: 533931.9905952082. RMSE_train: 0.8168689371761546\n",
      "SE_test:  156697.58981283335. RMSE_test:  0.8850578325476484\n",
      "\n",
      "Iterations: 75\n",
      "SE_train: 533812.167992895. RMSE_train: 0.8167772730170926\n",
      "SE_test:  156686.85527650817. RMSE_test:  0.885027516675589\n",
      "\n",
      "Iterations: 76\n",
      "SE_train: 533696.9747555766. RMSE_train: 0.8166891406174484\n",
      "SE_test:  156676.61782184424. RMSE_test:  0.8849986036660626\n",
      "\n",
      "Iterations: 77\n",
      "SE_train: 533586.1850609008. RMSE_train: 0.8166043683206148\n",
      "SE_test:  156666.84578056773. RMSE_test:  0.8849710042135772\n",
      "\n",
      "Iterations: 78\n",
      "SE_train: 533479.5862561906. RMSE_train: 0.8165227944342606\n",
      "SE_test:  156657.50981016833. RMSE_test:  0.8849446355643944\n",
      "\n",
      "Iterations: 79\n",
      "SE_train: 533376.9779243338. RMSE_train: 0.8164442665270819\n",
      "SE_test:  156648.58271143388. RMSE_test:  0.8849194210033631\n",
      "\n",
      "Iterations: 80\n",
      "SE_train: 533278.1710307647. RMSE_train: 0.8163686407863469\n",
      "SE_test:  156640.03926126764. RMSE_test:  0.8848952893836296\n",
      "\n",
      "Iterations: 81\n",
      "SE_train: 533182.9871453007. RMSE_train: 0.8162957814316092\n",
      "SE_test:  156631.85605954996. RMSE_test:  0.8848721746957646\n",
      "\n",
      "Iterations: 82\n",
      "SE_train: 533091.2577297139. RMSE_train: 0.8162255601777453\n",
      "SE_test:  156624.0113885571. RMSE_test:  0.8848500156721405\n",
      "\n",
      "Iterations: 83\n",
      "SE_train: 533002.8234864462. RMSE_train: 0.8161578557439144\n",
      "SE_test:  156616.48508384195. RMSE_test:  0.8848287554234959\n",
      "\n",
      "Iterations: 84\n",
      "SE_train: 532917.5337612638. RMSE_train: 0.8160925534030308\n",
      "SE_test:  156609.2584154824. RMSE_test:  0.8848083411046214\n",
      "\n",
      "Iterations: 85\n",
      "SE_train: 532835.2459949863. RMSE_train: 0.8160295445681088\n",
      "SE_test:  156602.31397865654. RMSE_test:  0.8847887236062468\n",
      "\n",
      "Iterations: 86\n",
      "SE_train: 532755.8252186467. RMSE_train: 0.8159687264112438\n",
      "SE_test:  156595.63559278645. RMSE_test:  0.88476985727101\n",
      "\n",
      "Iterations: 87\n",
      "SE_train: 532679.1435884434. RMSE_train: 0.8159100015125044\n",
      "SE_test:  156589.2082084243. RMSE_test:  0.8847516996311855\n",
      "\n",
      "Iterations: 88\n",
      "SE_train: 532605.0799558957. RMSE_train: 0.8158532775352881\n",
      "SE_test:  156583.01782116978. RMSE_test:  0.8847342111661815\n",
      "\n",
      "Iterations: 89\n",
      "SE_train: 532533.5194697387. RMSE_train: 0.8157984669255387\n",
      "SE_test:  156577.051392112. RMSE_test:  0.8847173550783848\n",
      "\n",
      "Iterations: 90\n",
      "SE_train: 532464.3532067263. RMSE_train: 0.8157454866327033\n",
      "SE_test:  156571.2967741815. RMSE_test:  0.8847010970856285\n",
      "\n",
      "Iterations: 91\n",
      "SE_train: 532397.4778279928. RMSE_train: 0.8156942578499068\n",
      "SE_test:  156565.74264399838. RMSE_test:  0.884685405229122\n",
      "\n",
      "Iterations: 92\n",
      "SE_train: 532332.7952590315. RMSE_train: 0.8156447057718931\n",
      "SE_test:  156560.37843877188. RMSE_test:  0.8846702496955952\n",
      "\n",
      "Iterations: 93\n",
      "SE_train: 532270.2123910366. RMSE_train: 0.815596759369039\n",
      "SE_test:  156555.19429790936. RMSE_test:  0.8846556026526986\n",
      "\n",
      "Iterations: 94\n",
      "SE_train: 532209.6408012165. RMSE_train: 0.8155503511756383\n",
      "SE_test:  156550.18100896155. RMSE_test:  0.88464143809661\n",
      "\n",
      "Iterations: 95\n",
      "SE_train: 532150.9964912527. RMSE_train: 0.8155054170918496\n",
      "SE_test:  156545.32995765697. RMSE_test:  0.8846277317111553\n",
      "\n",
      "Iterations: 96\n",
      "SE_train: 532094.1996416087. RMSE_train: 0.8154618961975718\n",
      "SE_test:  156540.63308169032. RMSE_test:  0.8846144607375022\n",
      "\n",
      "Iterations: 97\n",
      "SE_train: 532039.1743810156. RMSE_train: 0.8154197305777536\n",
      "SE_test:  156536.0828280501. RMSE_test:  0.8846016038538222\n",
      "\n",
      "Iterations: 98\n",
      "SE_train: 531985.8485695337. RMSE_train: 0.8153788651579279\n",
      "SE_test:  156531.67211365426. RMSE_test:  0.8845891410642747\n",
      "\n",
      "Iterations: 99\n",
      "SE_train: 531934.1535945975. RMSE_train: 0.815339247549535\n",
      "SE_test:  156527.39428908445. RMSE_test:  0.8845770535967195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "U_0,M_0,SE_0 = mat_fac(folded_data,X,0) # Done with 0.01,0.1,10,100. Gives 0.884577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(np.arange(0,len(SE_0)),SE_0,'.')\n",
    "plt.xlabel('Iterations'); plt.ylabel('Total error')\n",
    "plt.title('Total error over time')\n",
    "plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example plot which shows the error over time. Notice the rapid decrease in the beginning and the slow decrease of the error in the final iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
